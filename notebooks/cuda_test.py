import pandas as pd
from datasets import Dataset
from transformers import AutoTokenizer, BertForSequenceClassification, TrainingArguments, Trainer
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
import numpy as np
import re
import torch
from transformers import DataCollatorWithPadding
from transformers import PreTrainedTokenizerFast
import os
os.environ['CUDA_LAUNCH_BLOCKING'] = "1"  # ì •í™•í•œ ì˜¤ë¥˜ ìœ„ì¹˜ ì¶”ì [1][6]

def remove_prefix_and_normalize(text):
    import re
    text = re.sub(r"^(ì •ë‹µ|ë‹µë³€|ê·¼ê±° ë¬¸ì¥|ê·¼ê±°)\s*[:ï¼š]?\s*", "", text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def replace_template_tags(text):
    tags = {
        "[ì¸ë¬¼]": "íŠ¹ì • ì¸ë¬¼",
        "[ì§ì—…]": "íŠ¹ì • ì§ì—…",
        "[ì¥ì†Œ]": "íŠ¹ì • ì¥ì†Œ",
        "[ê¸°ì—….ê¸°íƒ€]": "ê¸°íƒ€ê¸°ì—…",
        "[ì œí’ˆ]": "íŠ¹ì • ì œí’ˆ",
        "[ì •ë‹¹]": "íŠ¹ì • ì •ë‹¹",
        "[ì¢…êµ]": "íŠ¹ì • ì¢…êµ",
        "[ìŒì‹]": "íŠ¹ì • ìŒì‹",
        "[ê¸°ê´€]": "íŠ¹ì • ê¸°ê´€",
        "[ì‹ ë…]": "íŠ¹ì • ì‹ ë…",
        "[ì´ë¦„]": "íŠ¹ì • ì´ë¦„",
        "[ì§€ì—­]": "íŠ¹ì • ì§€ì—­",
    }
    for k, v in tags.items():
        text = text.replace(k, v)
    return text

from konlpy.tag import Okt
okt = Okt()

def extract_key_tokens(text):
    return [w for w, p in okt.pos(text) if p in ['Noun', 'Verb', 'Adjective']]

stop_words = {'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜'}

def remove_stopwords(tokens):
    return [w for w in tokens if w not in stop_words]

def normalize_symbols(text):
    text = re.sub(r"([ã…‹ã…ã…œã… ])\1{2,}", r"\1\1", text)
    text = text.replace("?", " <QUESTION>").replace("!", " <EXCLAMATION>")
    return text

def full_preprocess(text):
    text = remove_prefix_and_normalize(text)
    text = replace_template_tags(text)
    text = normalize_symbols(text)
    tokens = extract_key_tokens(text)
    tokens = remove_stopwords(tokens)
    return " ".join(tokens)

# âœ… 1. ë°ì´í„° ë¡œë“œ & ì „ì²˜ë¦¬ ì ìš©
train_df = pd.read_csv(r"C:\Users\MSI\Desktop\SpamGuard-MLOps\data\train\merged_train.csv")
test_df = pd.read_csv(r"C:\Users\MSI\Desktop\SpamGuard-MLOps\data\test\merged_test.csv")

# ì „ì²˜ë¦¬ í•¨ìˆ˜ ì ìš© (full_preprocessëŠ” ë„¤ê°€ ë§Œë“  í•¨ìˆ˜ ê·¸ëŒ€ë¡œ)
train_df["text"] = train_df["text"].map(full_preprocess)
test_df["text"] = test_df["text"].map(full_preprocess)

# âœ… 2. Label Encoding
le = LabelEncoder()
train_df["label"] = le.fit_transform(train_df["label"]).astype("int64")
test_df ["label"] = le.transform(test_df ["label"]).astype("int64")
num_labels = len(le.classes_)

# âœ… 3. Huggingface Dataset ë³€í™˜
train_dataset = Dataset.from_pandas(train_df[["text", "label"]])
test_dataset = Dataset.from_pandas(test_df[["text", "label"]])

# âœ… 4. Tokenizer ë¡œë”© ë° í† í¬ë‚˜ì´ì§•
MODEL_NAME = "monologg/kobert"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False, trust_remote_code=True)

if tokenizer.pad_token is None:                     # KoBERTì—” padê°€ ì—†ìŒ
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})

def tok_fn(batch):
    return tokenizer(batch["text"],
               truncation=True,
               padding="max_length",
               max_length=128)

train_dataset = train_dataset.map(tok_fn, batched=True, remove_columns=["text"])
test_dataset  = test_dataset .map(tok_fn, batched=True, remove_columns=["text"])


# âœ… 5. ëª¨ë¸ ë¡œë”© (num_labels ì¤‘ìš”!)
model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)
model.resize_token_embeddings(len(tokenizer))       # pad í† í° ì¶”ê°€ëìœ¼ë¯€ë¡œ í•„ìˆ˜


data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)
# âœ… 6. TrainingArguments ì„¤ì •
training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_dir="./logs",
    logging_steps=50,
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    load_best_model_at_end=True,
    save_total_limit=1,
    report_to="none"  # wandb ì—°ë™ X
)

# âœ… 7. Trainer êµ¬ì„±
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    data_collator=data_collator,
)

# âœ… 8. í•™ìŠµ ì‹œì‘
trainer.train()

# âœ… 9. ì˜ˆì¸¡ ë° í‰ê°€
preds = trainer.predict(test_dataset)
y_pred = np.argmax(preds.predictions, axis=1)
y_true = test_df["label"].values

# âœ… 10. ëª¨ë¸ ì €ì¥
model.save_pretrained("./saved_model/kobert")
tokenizer.save_pretrained("./saved_model/kobert")


print("\nğŸ“Š Classification Report:")
print(classification_report(y_true, y_pred, target_names=le.classes_))
