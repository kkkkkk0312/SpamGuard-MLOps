{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df017282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 'TL_1. 질의응답 데이터.zip' → 'TL_1. 질의응답 데이터/' 해제 완료\n",
      "✅ 'TL_2. 유해질의 데이터_범죄.zip' → 'TL_2. 유해질의 데이터_범죄/' 해제 완료\n",
      "✅ 'TL_2. 유해질의 데이터_비난혐오차별.zip' → 'TL_2. 유해질의 데이터_비난혐오차별/' 해제 완료\n",
      "✅ 'TL_2. 유해질의 데이터_선정.zip' → 'TL_2. 유해질의 데이터_선정/' 해제 완료\n",
      "✅ 'TL_2. 유해질의 데이터_스팸및광고.zip' → 'TL_2. 유해질의 데이터_스팸및광고/' 해제 완료\n",
      "✅ 'TL_2. 유해질의 데이터_욕설.zip' → 'TL_2. 유해질의 데이터_욕설/' 해제 완료\n",
      "✅ 'TL_2. 유해질의 데이터_폭력.zip' → 'TL_2. 유해질의 데이터_폭력/' 해제 완료\n",
      "✅ 'TL_2. 유해질의 데이터_허위정보및루머.zip' → 'TL_2. 유해질의 데이터_허위정보및루머/' 해제 완료\n",
      "✅ 'VL_1. 말뭉치 데이터.zip' → 'VL_1. 말뭉치 데이터/' 해제 완료\n",
      "✅ 'VL_2. 유해질의 데이터_범죄.zip' → 'VL_2. 유해질의 데이터_범죄/' 해제 완료\n",
      "✅ 'VL_2. 유해질의 데이터_비난혐오차별.zip' → 'VL_2. 유해질의 데이터_비난혐오차별/' 해제 완료\n",
      "✅ 'VL_2. 유해질의 데이터_선정.zip' → 'VL_2. 유해질의 데이터_선정/' 해제 완료\n",
      "✅ 'VL_2. 유해질의 데이터_스팸및광고.zip' → 'VL_2. 유해질의 데이터_스팸및광고/' 해제 완료\n",
      "✅ 'VL_2. 유해질의 데이터_욕설.zip' → 'VL_2. 유해질의 데이터_욕설/' 해제 완료\n",
      "✅ 'VL_2. 유해질의 데이터_폭력.zip' → 'VL_2. 유해질의 데이터_폭력/' 해제 완료\n",
      "✅ 'VL_2. 유해질의 데이터_허위정보및루머.zip' → 'VL_2. 유해질의 데이터_허위정보및루머/' 해제 완료\n",
      "✅ 모든 zip 해제 완료\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "def unzip_into_subfolders(base_dir):\n",
    "    \"\"\"\n",
    "    base_dir 안의 모든 zip 파일을 같은 이름의 폴더로 풀기.\n",
    "    예: base_dir/욕설.zip → base_dir/욕설/...\n",
    "    \"\"\"\n",
    "    for fname in os.listdir(base_dir):\n",
    "        if not fname.endswith(\".zip\"):\n",
    "            continue\n",
    "\n",
    "        zip_path = os.path.join(base_dir, fname)\n",
    "        folder_name = fname.replace(\".zip\", \"\").strip()\n",
    "        target_path = os.path.join(base_dir, folder_name)\n",
    "        os.makedirs(target_path, exist_ok=True)\n",
    "\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(target_path)\n",
    "            print(f\"✅ '{fname}' → '{folder_name}/' 해제 완료\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unzip_into_subfolders(r\"C:\\Users\\MSI\\Desktop\\SpamGuard-MLOps\\data\\train\\02.라벨링데이터\")\n",
    "\n",
    "    unzip_into_subfolders(r\"C:\\Users\\MSI\\Desktop\\SpamGuard-MLOps\\data\\test\\02.라벨링데이터\")\n",
    "\n",
    "    print(\"✅ 모든 zip 해제 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84185924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def load_normal_from_corpus(json_path, limit=None):\n",
    "    rows = []\n",
    "    with open(json_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        data = json.load(f)[\"data\"]\n",
    "\n",
    "    for i, entry in enumerate(data):\n",
    "        for label in entry.get(\"labels\", []):\n",
    "            response = label.get(\"response\", \"\").strip()\n",
    "            if response:\n",
    "                rows.append({\"text\": response, \"label\": \"정상\"})\n",
    "        if limit and i >= limit:\n",
    "            break\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df = load_normal_from_corpus(r\"C:\\Users\\MSI\\Desktop\\SpamGuard-MLOps\\data\\test\\02.라벨링데이터\\VL_1. 말뭉치 데이터\\validation(yes,no).json\")\n",
    "df.to_csv(\"data/normal_corpus1.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c41da6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def load_normal_from_corpus(json_path, limit=None):\n",
    "    rows = []\n",
    "    with open(json_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        data = json.load(f)[\"data\"]\n",
    "\n",
    "    for i, entry in enumerate(data):\n",
    "        for label in entry.get(\"labels\", []):\n",
    "            response = label.get(\"response\", \"\").strip()\n",
    "            if response:\n",
    "                rows.append({\"text\": response, \"label\": \"정상\"})\n",
    "        if limit and i >= limit:\n",
    "            break\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df = load_normal_from_corpus(r\"C:\\Users\\MSI\\Desktop\\SpamGuard-MLOps\\data\\test\\02.라벨링데이터\\VL_1. 말뭉치 데이터\\validation(의문사).json\")\n",
    "df.to_csv(\"data/normal_corpus2.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a27b5a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def load_normal_from_corpus(json_path, limit=None):\n",
    "    rows = []\n",
    "    with open(json_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        data = json.load(f)[\"data\"]\n",
    "\n",
    "    for i, entry in enumerate(data):\n",
    "        for label in entry.get(\"labels\", []):\n",
    "            response = label.get(\"response\", \"\").strip()\n",
    "            if response:\n",
    "                rows.append({\"text\": response, \"label\": \"정상\"})\n",
    "        if limit and i >= limit:\n",
    "            break\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df = load_normal_from_corpus(r\"C:\\Users\\MSI\\Desktop\\SpamGuard-MLOps\\data\\train\\TL_1. 질의응답 데이터\\training(yes,no).json\")\n",
    "df.to_csv(\"data/train_normal_corpus1.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "058f72bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def load_normal_from_corpus(json_path, limit=None):\n",
    "    rows = []\n",
    "    with open(json_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        data = json.load(f)[\"data\"]\n",
    "\n",
    "    for i, entry in enumerate(data):\n",
    "        for label in entry.get(\"labels\", []):\n",
    "            response = label.get(\"response\", \"\").strip()\n",
    "            if response:\n",
    "                rows.append({\"text\": response, \"label\": \"정상\"})\n",
    "        if limit and i >= limit:\n",
    "            break\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df = load_normal_from_corpus(r\"C:\\Users\\MSI\\Desktop\\SpamGuard-MLOps\\data\\train\\TL_1. 질의응답 데이터\\training(의문사).json\")\n",
    "df.to_csv(\"data/train_normal_corpus2.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8c8a6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "LABEL_MAP = {\n",
    "    1: \"비난혐오차별\",\n",
    "    2: \"선정\",\n",
    "    3: \"스팸및광고\",\n",
    "    4: \"욕설\",\n",
    "    5: \"폭력\",\n",
    "    6: \"허위정보및루머\",\n",
    "    7: \"범죄\"\n",
    "}\n",
    "\n",
    "def collect_labeled_data(base_dir):\n",
    "    rows = []\n",
    "    for category in os.listdir(base_dir):\n",
    "        cat_path = os.path.join(base_dir, category)\n",
    "        if not os.path.isdir(cat_path): continue\n",
    "        for fname in os.listdir(cat_path):\n",
    "            if not fname.endswith(\".json\"): continue\n",
    "            try:\n",
    "                with open(os.path.join(cat_path, fname), \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)[\"data\"][0]\n",
    "                    text = data[\"instruct_text\"].strip()\n",
    "                    level1 = data[\"labels\"][0][\"level1_type\"]\n",
    "                    label = LABEL_MAP.get(level1, \"기타\")\n",
    "                    rows.append({\"text\": text, \"label\": label})\n",
    "            except Exception as e:\n",
    "                print(f\"[에러: {fname}] {e}\")\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# 실행 예시\n",
    "train_df = collect_labeled_data(r\"C:\\Users\\MSI\\Desktop\\SpamGuard-MLOps\\data\\train\\02.라벨링데이터\")\n",
    "test_df = collect_labeled_data(r\"C:\\Users\\MSI\\Desktop\\SpamGuard-MLOps\\data\\test\\02.라벨링데이터\")\n",
    "\n",
    "train_df.to_csv(\"data/harmful_train.csv\", index=False)\n",
    "test_df.to_csv(\"data/harmful_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76644267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 병합 완료: 총 2112개 문장 → data/merged_test.csv 저장됨\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 각 데이터 불러오기\n",
    "harmful = pd.read_csv(\"data/harmful_test.csv\")\n",
    "normal1 = pd.read_csv(\"data/normal_corpus1.csv\")\n",
    "normal2 = pd.read_csv(\"data/normal_corpus2.csv\")\n",
    "\n",
    "# 유해 문장 수\n",
    "n_harmful = len(harmful)\n",
    "\n",
    "# 정상은 반반 샘플링 (총합이 유해와 동일하게)\n",
    "n_each = n_harmful // 2\n",
    "normal1_sampled = normal1.sample(n=n_each, random_state=42)\n",
    "normal2_sampled = normal2.sample(n=n_harmful - n_each, random_state=42)  # odd num 대응\n",
    "\n",
    "# 병합\n",
    "merged = pd.concat([harmful, normal1_sampled, normal2_sampled], ignore_index=True)\n",
    "\n",
    "# 셔플 (모델 학습 안정화를 위해)\n",
    "merged = merged.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# 저장\n",
    "merged.to_csv(\"data/merged_train.csv\", index=False)\n",
    "\n",
    "print(f\"✅ 병합 완료: 총 {len(merged)}개 문장 → data/merged_train.csv 저장됨\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2404b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 각 데이터 불러오기\n",
    "harmful = pd.read_csv(\"data/harmful_train.csv\")\n",
    "normal1 = pd.read_csv(\"data/normal_corpus.csv\")\n",
    "normal2 = pd.read_csv(\"data/normal_corpus2.csv\")\n",
    "\n",
    "# 유해 문장 수\n",
    "n_harmful = len(harmful)\n",
    "\n",
    "# 정상은 반반 샘플링 (총합이 유해와 동일하게)\n",
    "n_each = n_harmful // 2\n",
    "normal1_sampled = normal1.sample(n=n_each, random_state=42)\n",
    "normal2_sampled = normal2.sample(n=n_harmful - n_each, random_state=42)  # odd num 대응\n",
    "\n",
    "# 병합\n",
    "merged = pd.concat([harmful, normal1_sampled, normal2_sampled], ignore_index=True)\n",
    "\n",
    "# 셔플 (모델 학습 안정화를 위해)\n",
    "merged = merged.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# 저장\n",
    "merged.to_csv(\"data/merged_train.csv\", index=False)\n",
    "\n",
    "print(f\"✅ 병합 완료: 총 {len(merged)}개 문장 → data/merged_train.csv 저장됨\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
