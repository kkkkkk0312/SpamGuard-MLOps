{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df017282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 'TL_1. 질의응답 데이터.zip' → 'TL_1. 질의응답 데이터/' 해제 완료\n",
      "✅ 'TL_2. 유해질의 데이터_범죄.zip' → 'TL_2. 유해질의 데이터_범죄/' 해제 완료\n",
      "✅ 'TL_2. 유해질의 데이터_비난혐오차별.zip' → 'TL_2. 유해질의 데이터_비난혐오차별/' 해제 완료\n",
      "✅ 'TL_2. 유해질의 데이터_선정.zip' → 'TL_2. 유해질의 데이터_선정/' 해제 완료\n",
      "✅ 'TL_2. 유해질의 데이터_스팸및광고.zip' → 'TL_2. 유해질의 데이터_스팸및광고/' 해제 완료\n",
      "✅ 'TL_2. 유해질의 데이터_욕설.zip' → 'TL_2. 유해질의 데이터_욕설/' 해제 완료\n",
      "✅ 'TL_2. 유해질의 데이터_폭력.zip' → 'TL_2. 유해질의 데이터_폭력/' 해제 완료\n",
      "✅ 'TL_2. 유해질의 데이터_허위정보및루머.zip' → 'TL_2. 유해질의 데이터_허위정보및루머/' 해제 완료\n",
      "✅ 'VL_1. 말뭉치 데이터.zip' → 'VL_1. 말뭉치 데이터/' 해제 완료\n",
      "✅ 'VL_2. 유해질의 데이터_범죄.zip' → 'VL_2. 유해질의 데이터_범죄/' 해제 완료\n",
      "✅ 'VL_2. 유해질의 데이터_비난혐오차별.zip' → 'VL_2. 유해질의 데이터_비난혐오차별/' 해제 완료\n",
      "✅ 'VL_2. 유해질의 데이터_선정.zip' → 'VL_2. 유해질의 데이터_선정/' 해제 완료\n",
      "✅ 'VL_2. 유해질의 데이터_스팸및광고.zip' → 'VL_2. 유해질의 데이터_스팸및광고/' 해제 완료\n",
      "✅ 'VL_2. 유해질의 데이터_욕설.zip' → 'VL_2. 유해질의 데이터_욕설/' 해제 완료\n",
      "✅ 'VL_2. 유해질의 데이터_폭력.zip' → 'VL_2. 유해질의 데이터_폭력/' 해제 완료\n",
      "✅ 'VL_2. 유해질의 데이터_허위정보및루머.zip' → 'VL_2. 유해질의 데이터_허위정보및루머/' 해제 완료\n",
      "✅ 모든 zip 해제 완료\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "def unzip_into_subfolders(base_dir):\n",
    "    \"\"\"\n",
    "    base_dir 안의 모든 zip 파일을 같은 이름의 폴더로 풀기.\n",
    "    예: base_dir/욕설.zip → base_dir/욕설/...\n",
    "    \"\"\"\n",
    "    for fname in os.listdir(base_dir):\n",
    "        if not fname.endswith(\".zip\"):\n",
    "            continue\n",
    "\n",
    "        zip_path = os.path.join(base_dir, fname)\n",
    "        folder_name = fname.replace(\".zip\", \"\").strip()\n",
    "        target_path = os.path.join(base_dir, folder_name)\n",
    "        os.makedirs(target_path, exist_ok=True)\n",
    "\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(target_path)\n",
    "            print(f\"✅ '{fname}' → '{folder_name}/' 해제 완료\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unzip_into_subfolders(r\"C:\\Users\\MSI\\Desktop\\SpamGuard-MLOps\\data\\train\\02.라벨링데이터\")\n",
    "\n",
    "    unzip_into_subfolders(r\"C:\\Users\\MSI\\Desktop\\SpamGuard-MLOps\\data\\test\\02.라벨링데이터\")\n",
    "\n",
    "    print(\"✅ 모든 zip 해제 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84185924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def load_normal_from_corpus(json_path, limit=None):\n",
    "    rows = []\n",
    "    with open(json_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        data = json.load(f)[\"data\"]\n",
    "\n",
    "    for i, entry in enumerate(data):\n",
    "        for label in entry.get(\"labels\", []):\n",
    "            response = label.get(\"response\", \"\").strip()\n",
    "            if response:\n",
    "                rows.append({\"text\": response, \"label\": \"정상\"})\n",
    "        if limit and i >= limit:\n",
    "            break\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df = load_normal_from_corpus(r\"C:\\Users\\MSI\\Desktop\\SpamGuard-MLOps\\data\\test\\02.라벨링데이터\\VL_1. 말뭉치 데이터\\validation(yes,no).json\")\n",
    "df.to_csv(\"data/normal_corpus1.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c41da6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def load_normal_from_corpus(json_path, limit=None):\n",
    "    rows = []\n",
    "    with open(json_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        data = json.load(f)[\"data\"]\n",
    "\n",
    "    for i, entry in enumerate(data):\n",
    "        for label in entry.get(\"labels\", []):\n",
    "            response = label.get(\"response\", \"\").strip()\n",
    "            if response:\n",
    "                rows.append({\"text\": response, \"label\": \"정상\"})\n",
    "        if limit and i >= limit:\n",
    "            break\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df = load_normal_from_corpus(r\"C:\\Users\\MSI\\Desktop\\SpamGuard-MLOps\\data\\test\\02.라벨링데이터\\VL_1. 말뭉치 데이터\\validation(의문사).json\")\n",
    "df.to_csv(\"data/normal_corpus2.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a27b5a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def load_normal_from_corpus(json_path, limit=None):\n",
    "    rows = []\n",
    "    with open(json_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        data = json.load(f)[\"data\"]\n",
    "\n",
    "    for i, entry in enumerate(data):\n",
    "        for label in entry.get(\"labels\", []):\n",
    "            response = label.get(\"response\", \"\").strip()\n",
    "            if response:\n",
    "                rows.append({\"text\": response, \"label\": \"정상\"})\n",
    "        if limit and i >= limit:\n",
    "            break\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df = load_normal_from_corpus(r\"C:\\Users\\MSI\\Desktop\\SpamGuard-MLOps\\data\\train\\TL_1. 질의응답 데이터\\training(yes,no).json\")\n",
    "df.to_csv(\"data/train_normal_corpus1.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "058f72bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def load_normal_from_corpus(json_path, limit=None):\n",
    "    rows = []\n",
    "    with open(json_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        data = json.load(f)[\"data\"]\n",
    "\n",
    "    for i, entry in enumerate(data):\n",
    "        for label in entry.get(\"labels\", []):\n",
    "            response = label.get(\"response\", \"\").strip()\n",
    "            if response:\n",
    "                rows.append({\"text\": response, \"label\": \"정상\"})\n",
    "        if limit and i >= limit:\n",
    "            break\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df = load_normal_from_corpus(r\"C:\\Users\\MSI\\Desktop\\SpamGuard-MLOps\\data\\train\\TL_1. 질의응답 데이터\\training(의문사).json\")\n",
    "df.to_csv(\"data/train_normal_corpus2.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8c8a6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "LABEL_MAP = {\n",
    "    1: \"비난혐오차별\",\n",
    "    2: \"선정\",\n",
    "    3: \"스팸및광고\",\n",
    "    4: \"욕설\",\n",
    "    5: \"폭력\",\n",
    "    6: \"허위정보및루머\",\n",
    "    7: \"범죄\"\n",
    "}\n",
    "\n",
    "def collect_labeled_data(base_dir):\n",
    "    rows = []\n",
    "    for category in os.listdir(base_dir):\n",
    "        cat_path = os.path.join(base_dir, category)\n",
    "        if not os.path.isdir(cat_path): continue\n",
    "        for fname in os.listdir(cat_path):\n",
    "            if not fname.endswith(\".json\"): continue\n",
    "            try:\n",
    "                with open(os.path.join(cat_path, fname), \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)[\"data\"][0]\n",
    "                    text = data[\"instruct_text\"].strip()\n",
    "                    level1 = data[\"labels\"][0][\"level1_type\"]\n",
    "                    label = LABEL_MAP.get(level1, \"기타\")\n",
    "                    rows.append({\"text\": text, \"label\": label})\n",
    "            except Exception as e:\n",
    "                print(f\"[에러: {fname}] {e}\")\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# 실행 예시\n",
    "train_df = collect_labeled_data(r\"C:\\Users\\MSI\\Desktop\\SpamGuard-MLOps\\data\\train\\02.라벨링데이터\")\n",
    "test_df = collect_labeled_data(r\"C:\\Users\\MSI\\Desktop\\SpamGuard-MLOps\\data\\test\\02.라벨링데이터\")\n",
    "\n",
    "train_df.to_csv(\"data/harmful_train.csv\", index=False)\n",
    "test_df.to_csv(\"data/harmful_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76644267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 병합 완료: 총 2112개 문장 → data/merged_test.csv 저장됨\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 각 데이터 불러오기\n",
    "harmful = pd.read_csv(\"data/harmful_test.csv\")\n",
    "normal1 = pd.read_csv(\"data/normal_corpus1.csv\")\n",
    "normal2 = pd.read_csv(\"data/normal_corpus2.csv\")\n",
    "\n",
    "# 유해 문장 수\n",
    "n_harmful = len(harmful)\n",
    "\n",
    "# 정상은 반반 샘플링 (총합이 유해와 동일하게)\n",
    "n_each = n_harmful // 2\n",
    "normal1_sampled = normal1.sample(n=n_each, random_state=42)\n",
    "normal2_sampled = normal2.sample(n=n_harmful - n_each, random_state=42)  # odd num 대응\n",
    "\n",
    "# 병합\n",
    "merged = pd.concat([harmful, normal1_sampled, normal2_sampled], ignore_index=True)\n",
    "\n",
    "# 셔플 (모델 학습 안정화를 위해)\n",
    "merged = merged.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# 저장\n",
    "merged.to_csv(\"data/merged_test.csv\", index=False)\n",
    "\n",
    "print(f\"✅ 병합 완료: 총 {len(merged)}개 문장 → data/merged_test.csv 저장됨\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c2404b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 병합 완료: 총 16896개 문장 → data/merged_train.csv 저장됨\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 각 데이터 불러오기\n",
    "harmful = pd.read_csv(\"data/harmful_train.csv\")\n",
    "normal1 = pd.read_csv(\"data/train_normal_corpus1.csv\")\n",
    "normal2 = pd.read_csv(\"data/train_normal_corpus2.csv\")\n",
    "\n",
    "# 유해 문장 수\n",
    "n_harmful = len(harmful)\n",
    "\n",
    "# 정상은 반반 샘플링 (총합이 유해와 동일하게)\n",
    "n_each = n_harmful // 2\n",
    "normal1_sampled = normal1.sample(n=n_each, random_state=42)\n",
    "normal2_sampled = normal2.sample(n=n_harmful - n_each, random_state=42)  # odd num 대응\n",
    "\n",
    "# 병합\n",
    "merged = pd.concat([harmful, normal1_sampled, normal2_sampled], ignore_index=True)\n",
    "\n",
    "# 셔플 (모델 학습 안정화를 위해)\n",
    "merged = merged.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# 저장\n",
    "merged.to_csv(\"data/merged_train.csv\", index=False)\n",
    "\n",
    "print(f\"✅ 병합 완료: 총 {len(merged)}개 문장 → data/merged_train.csv 저장됨\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f24a23ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MSI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\MSI\\.cache\\huggingface\\hub\\models--monologg--kobert. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "100%|██████████| 16896/16896 [14:58<00:00, 18.80it/s]\n",
      "100%|██████████| 2112/2112 [01:52<00:00, 18.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          범죄       0.57      0.46      0.51       192\n",
      "      비난혐오차별       0.57      0.74      0.65       320\n",
      "          선정       0.43      0.45      0.44        96\n",
      "       스팸및광고       0.52      0.40      0.45        96\n",
      "          욕설       0.14      0.03      0.05        64\n",
      "          정상       0.99      1.00      0.99      1056\n",
      "          폭력       0.54      0.58      0.56       224\n",
      "     허위정보및루머       0.32      0.23      0.27        64\n",
      "\n",
      "    accuracy                           0.76      2112\n",
      "   macro avg       0.51      0.49      0.49      2112\n",
      "weighted avg       0.75      0.76      0.75      2112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ✅ KoBERT 로드\n",
    "MODEL_NAME = \"monologg/kobert\"\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BertModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# ✅ 텍스트 → KoBERT 임베딩 추출\n",
    "def get_embedding(text, max_len=128):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_len, padding=\"max_length\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] 토큰 벡터\n",
    "    return cls_embedding.squeeze().cpu().numpy()\n",
    "\n",
    "# ✅ 전체 벡터화\n",
    "def embed_corpus(texts):\n",
    "    return [get_embedding(text) for text in tqdm(texts)]\n",
    "\n",
    "# ✅ 데이터 로드\n",
    "train_df = pd.read_csv(r\"C:\\Users\\MSI\\Desktop\\SpamGuard-MLOps\\data\\train\\merged_train.csv\")\n",
    "test_df = pd.read_csv(r\"C:\\Users\\MSI\\Desktop\\SpamGuard-MLOps\\data\\test\\merged_test.csv\")\n",
    "\n",
    "# ✅ 클래스 인코딩\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(train_df[\"label\"])\n",
    "y_test = le.transform(test_df[\"label\"])\n",
    "\n",
    "# ✅ 벡터 추출\n",
    "X_train = embed_corpus(train_df[\"text\"])\n",
    "X_test = embed_corpus(test_df[\"text\"])\n",
    "\n",
    "# ✅ 분류 모델 학습 (로지스틱 회귀)\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# ✅ 평가\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ebfd0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_prefix_and_normalize(text):\n",
    "    import re\n",
    "    text = re.sub(r\"^(정답|답변|근거 문장|근거)\\s*[:：]?\\s*\", \"\", text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3ff5214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_template_tags(text):\n",
    "    tags = {\n",
    "        \"[인물]\": \"특정 인물\",\n",
    "        \"[직업]\": \"특정 직업\",\n",
    "        \"[장소]\": \"특정 장소\",\n",
    "        \"[기업.기타]\": \"기타기업\",\n",
    "        \"[제품]\": \"특정 제품\",\n",
    "        \"[정당]\": \"특정 정당\",\n",
    "        \"[종교]\": \"특정 종교\",\n",
    "        \"[음식]\": \"특정 음식\",\n",
    "        \"[기관]\": \"특정 기관\",\n",
    "        \"[신념]\": \"특정 신념\",\n",
    "        \"[이름]\": \"특정 이름\",\n",
    "        \"[지역]\": \"특정 지역\",\n",
    "    }\n",
    "    for k, v in tags.items():\n",
    "        text = text.replace(k, v)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d11d5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "\n",
    "def extract_key_tokens(text):\n",
    "    return [w for w, p in okt.pos(text) if p in ['Noun', 'Verb', 'Adjective']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "229a61c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = {'은', '는', '이', '가', '을', '를', '의'}\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [w for w in tokens if w not in stop_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5817e7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_symbols(text):\n",
    "    text = re.sub(r\"([ㅋㅎㅜㅠ])\\1{2,}\", r\"\\1\\1\", text)\n",
    "    text = text.replace(\"?\", \" <QUESTION>\").replace(\"!\", \" <EXCLAMATION>\")\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f69a4ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_preprocess(text):\n",
    "    text = remove_prefix_and_normalize(text)\n",
    "    text = replace_template_tags(text)\n",
    "    text = normalize_symbols(text)\n",
    "    tokens = extract_key_tokens(text)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5627b241",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "100%|██████████| 16896/16896 [13:05<00:00, 21.51it/s]\n",
      "100%|██████████| 2112/2112 [01:38<00:00, 21.44it/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# ✅ GPU 세팅\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ✅ KoBERT 모델 로딩\n",
    "MODEL_NAME = \"monologg/kobert\"\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BertModel.from_pretrained(MODEL_NAME)\n",
    "model.eval().to(device)\n",
    "\n",
    "# ✅ 텍스트 → KoBERT 임베딩 함수\n",
    "def get_embedding(text, max_len=128):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_len, padding=\"max_length\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] 임베딩\n",
    "    return cls_embedding.squeeze().cpu().numpy()\n",
    "\n",
    "# ✅ 전체 벡터화 함수\n",
    "def embed_corpus(texts):\n",
    "    return [get_embedding(text) for text in tqdm(texts)]\n",
    "\n",
    "# ✅ 데이터 불러오기\n",
    "train_df = pd.read_csv(r\"C:\\Users\\MSI\\Desktop\\SpamGuard-MLOps\\data\\train\\merged_train.csv\")\n",
    "test_df = pd.read_csv(r\"C:\\Users\\MSI\\Desktop\\SpamGuard-MLOps\\data\\test\\merged_test.csv\")\n",
    "\n",
    "# ✅ 전처리 적용\n",
    "train_df[\"text\"] = train_df[\"text\"].map(full_preprocess)\n",
    "test_df[\"text\"] = test_df[\"text\"].map(full_preprocess)\n",
    "\n",
    "# ✅ 클래스 인코딩\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(train_df[\"label\"])\n",
    "y_test = le.transform(test_df[\"label\"])\n",
    "\n",
    "# ✅ 전처리된 텍스트로 임베딩 생성\n",
    "X_train = embed_corpus(train_df[\"text\"])\n",
    "X_test = embed_corpus(test_df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "143e4b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 모델: LogisticRegression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MSI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          범죄       0.62      0.47      0.54       192\n",
      "      비난혐오차별       0.59      0.67      0.63       320\n",
      "          선정       0.55      0.61      0.58        96\n",
      "       스팸및광고       0.60      0.56      0.58        96\n",
      "          욕설       0.33      0.20      0.25        64\n",
      "          정상       0.92      0.95      0.93      1056\n",
      "          폭력       0.51      0.54      0.52       224\n",
      "     허위정보및루머       0.69      0.45      0.55        64\n",
      "\n",
      "    accuracy                           0.75      2112\n",
      "   macro avg       0.60      0.56      0.57      2112\n",
      "weighted avg       0.74      0.75      0.74      2112\n",
      "\n",
      "\n",
      "🚀 모델: RandomForest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          범죄       0.46      0.24      0.32       192\n",
      "      비난혐오차별       0.43      0.47      0.45       320\n",
      "          선정       0.76      0.23      0.35        96\n",
      "       스팸및광고       0.67      0.08      0.15        96\n",
      "          욕설       0.00      0.00      0.00        64\n",
      "          정상       0.71      0.95      0.82      1056\n",
      "          폭력       0.39      0.36      0.37       224\n",
      "     허위정보및루머       0.83      0.08      0.14        64\n",
      "\n",
      "    accuracy                           0.62      2112\n",
      "   macro avg       0.53      0.30      0.32      2112\n",
      "weighted avg       0.59      0.62      0.57      2112\n",
      "\n",
      "\n",
      "🚀 모델: XGBoost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MSI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [14:16:22] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          범죄       0.57      0.42      0.48       192\n",
      "      비난혐오차별       0.53      0.65      0.58       320\n",
      "          선정       0.59      0.43      0.49        96\n",
      "       스팸및광고       0.52      0.34      0.41        96\n",
      "          욕설       0.31      0.06      0.10        64\n",
      "          정상       0.88      0.94      0.91      1056\n",
      "          폭력       0.42      0.53      0.47       224\n",
      "     허위정보및루머       0.90      0.28      0.43        64\n",
      "\n",
      "    accuracy                           0.71      2112\n",
      "   macro avg       0.59      0.46      0.49      2112\n",
      "weighted avg       0.70      0.71      0.70      2112\n",
      "\n",
      "\n",
      "🚀 모델: SVC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          범죄       0.56      0.52      0.54       192\n",
      "      비난혐오차별       0.59      0.68      0.63       320\n",
      "          선정       0.57      0.62      0.59        96\n",
      "       스팸및광고       0.59      0.55      0.57        96\n",
      "          욕설       0.30      0.22      0.25        64\n",
      "          정상       0.93      0.93      0.93      1056\n",
      "          폭력       0.53      0.52      0.52       224\n",
      "     허위정보및루머       0.67      0.41      0.50        64\n",
      "\n",
      "    accuracy                           0.75      2112\n",
      "   macro avg       0.59      0.56      0.57      2112\n",
      "weighted avg       0.74      0.75      0.74      2112\n",
      "\n",
      "\n",
      "🚀 모델: KNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MSI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] 지정된 파일을 찾을 수 없습니다\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\MSI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\MSI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\MSI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"c:\\Users\\MSI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          범죄       0.37      0.56      0.44       192\n",
      "      비난혐오차별       0.42      0.57      0.48       320\n",
      "          선정       0.39      0.44      0.41        96\n",
      "       스팸및광고       0.29      0.24      0.26        96\n",
      "          욕설       0.07      0.05      0.06        64\n",
      "          정상       0.97      0.79      0.87      1056\n",
      "          폭력       0.36      0.38      0.37       224\n",
      "     허위정보및루머       0.52      0.39      0.45        64\n",
      "\n",
      "    accuracy                           0.62      2112\n",
      "   macro avg       0.42      0.43      0.42      2112\n",
      "weighted avg       0.67      0.62      0.63      2112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 모델 목록\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=1000),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
    "    \"SVC\": SVC(kernel='linear', probability=True),  # 큰 데이터셋일 경우 느림\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "# 훈련 + 평가\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n🚀 모델: {name}\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f57e6ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 16896/16896 [00:02<00:00, 7872.55 examples/s]\n",
      "Map: 100%|██████████| 2112/2112 [00:00<00:00, 7172.64 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(8002, 768, padding_idx=1)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "\n",
    "\n",
    "# ✅ 1. 데이터 로드 & 전처리 적용\n",
    "train_df = pd.read_csv(r\"C:\\Users\\MSI\\Desktop\\SpamGuard-MLOps\\data\\train\\merged_train.csv\")\n",
    "test_df = pd.read_csv(r\"C:\\Users\\MSI\\Desktop\\SpamGuard-MLOps\\data\\test\\merged_test.csv\")\n",
    "\n",
    "# 전처리 함수 적용 (full_preprocess는 네가 만든 함수 그대로)\n",
    "train_df[\"text\"] = train_df[\"text\"].map(full_preprocess)\n",
    "test_df[\"text\"] = test_df[\"text\"].map(full_preprocess)\n",
    "\n",
    "# ✅ 2. Label Encoding\n",
    "le = LabelEncoder()\n",
    "train_df[\"label\"] = le.fit_transform(train_df[\"label\"]).astype(\"int64\")\n",
    "test_df [\"label\"] = le.transform(test_df [\"label\"]).astype(\"int64\")\n",
    "num_labels = len(le.classes_)\n",
    "\n",
    "# ✅ 3. Huggingface Dataset 변환\n",
    "train_dataset = Dataset.from_pandas(train_df[[\"text\", \"label\"]])\n",
    "test_dataset = Dataset.from_pandas(test_df[[\"text\", \"label\"]])\n",
    "\n",
    "# ✅ 4. Tokenizer 로딩 및 토크나이징\n",
    "MODEL_NAME = \"monologg/kobert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False, trust_remote_code=True)\n",
    "\n",
    "if tokenizer.pad_token is None:                     # KoBERT엔 pad가 없음\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "def tok_fn(batch):\n",
    "    return tokenizer(batch[\"text\"],\n",
    "               truncation=True,\n",
    "               padding=\"max_length\",\n",
    "               max_length=128)\n",
    "\n",
    "train_dataset = train_dataset.map(tok_fn, batched=True, remove_columns=[\"text\"])\n",
    "test_dataset  = test_dataset .map(tok_fn, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "\n",
    "# ✅ 5. 모델 로딩 (num_labels 중요!)\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "model.resize_token_embeddings(len(tokenizer))       # pad 토큰 추가됐으므로 필수\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "89c88a82",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m      6\u001b[39m training_args = TrainingArguments(\n\u001b[32m      7\u001b[39m     output_dir=\u001b[33m\"\u001b[39m\u001b[33m./results\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      8\u001b[39m     eval_strategy=\u001b[33m\"\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     report_to=\u001b[33m\"\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# wandb 연동 X\u001b[39;00m\n\u001b[32m     18\u001b[39m )\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# ✅ 7. Trainer 구성\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m trainer = \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# ✅ 8. 학습 시작\u001b[39;00m\n\u001b[32m     30\u001b[39m trainer.train()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:455\u001b[39m, in \u001b[36mTrainer.__init__\u001b[39m\u001b[34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[39m\n\u001b[32m    453\u001b[39m \u001b[38;5;28mself\u001b[39m.compute_loss_func = compute_loss_func\n\u001b[32m    454\u001b[39m \u001b[38;5;66;03m# Seed must be set before instantiating the model when using model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m enable_full_determinism(\u001b[38;5;28mself\u001b[39m.args.seed) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.full_determinism \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mset_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28mself\u001b[39m.hp_name = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    458\u001b[39m \u001b[38;5;28mself\u001b[39m.deepspeed = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer_utils.py:105\u001b[39m, in \u001b[36mset_seed\u001b[39m\u001b[34m(seed, deterministic)\u001b[39m\n\u001b[32m    103\u001b[39m np.random.seed(seed)\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m     torch.cuda.manual_seed_all(seed)\n\u001b[32m    107\u001b[39m     \u001b[38;5;66;03m# ^^ safe to call this function even if cuda is not available\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_compile.py:32\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     29\u001b[39m     disable_fn = torch._dynamo.disable(fn, recursive)\n\u001b[32m     30\u001b[39m     fn.__dynamo_disable = disable_fn\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    741\u001b[39m prior_skip_guard_eval_unsafe = set_skip_guard_eval_unsafe(\n\u001b[32m    742\u001b[39m     _is_skip_guard_eval_unsafe_stance()\n\u001b[32m    743\u001b[39m )\n\u001b[32m    744\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    747\u001b[39m     _maybe_set_eval_frame(prior)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\random.py:46\u001b[39m, in \u001b[36mmanual_seed\u001b[39m\u001b[34m(seed)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcuda\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.cuda._is_in_bad_fork():\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmanual_seed_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmps\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.mps._is_in_bad_fork():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\cuda\\random.py:127\u001b[39m, in \u001b[36mmanual_seed_all\u001b[39m\u001b[34m(seed)\u001b[39m\n\u001b[32m    124\u001b[39m         default_generator = torch.cuda.default_generators[i]\n\u001b[32m    125\u001b[39m         default_generator.manual_seed(seed)\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[43m_lazy_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_all\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\cuda\\__init__.py:249\u001b[39m, in \u001b[36m_lazy_call\u001b[39m\u001b[34m(callable, **kwargs)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_lazy_call\u001b[39m(\u001b[38;5;28mcallable\u001b[39m, **kwargs):\n\u001b[32m    248\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m         \u001b[38;5;28;43mcallable\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    251\u001b[39m         \u001b[38;5;66;03m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[39;00m\n\u001b[32m    252\u001b[39m         \u001b[38;5;66;03m# file system to get traceback info. Patch linecache or do something\u001b[39;00m\n\u001b[32m    253\u001b[39m         \u001b[38;5;66;03m# else here if this ends up being important.\u001b[39;00m\n\u001b[32m    254\u001b[39m         \u001b[38;5;28;01mglobal\u001b[39;00m _lazy_seed_tracker\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\cuda\\random.py:125\u001b[39m, in \u001b[36mmanual_seed_all.<locals>.cb\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(device_count()):\n\u001b[32m    124\u001b[39m     default_generator = torch.cuda.default_generators[i]\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     default_generator.manual_seed(seed)\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"  # 정확한 오류 위치 추적[1][6]\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n",
    "# ✅ 6. TrainingArguments 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\"  # wandb 연동 X\n",
    ")\n",
    "\n",
    "# ✅ 7. Trainer 구성\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# ✅ 8. 학습 시작\n",
    "trainer.train()\n",
    "\n",
    "# ✅ 9. 예측 및 평가\n",
    "preds = trainer.predict(test_dataset)\n",
    "y_pred = np.argmax(preds.predictions, axis=1)\n",
    "y_true = test_df[\"label\"].values\n",
    "\n",
    "print(\"\\n📊 Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b82a8ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA 사용 가능? True\n",
      "현재 GPU 수: 1\n",
      "사용 중인 디바이스: 0\n",
      "디바이스 이름: NVIDIA GeForce RTX 4070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA 사용 가능?\", torch.cuda.is_available())\n",
    "print(\"현재 GPU 수:\", torch.cuda.device_count())\n",
    "print(\"사용 중인 디바이스:\", torch.cuda.current_device())\n",
    "print(\"디바이스 이름:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "58ba6224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ train_df label 타입: int64\n",
      "✅ max label: 7\n",
      "✅ unique label 수: 8\n"
     ]
    }
   ],
   "source": [
    "print(\"✅ train_df label 타입:\", train_df[\"label\"].dtype)\n",
    "print(\"✅ max label:\", train_df[\"label\"].max())\n",
    "print(\"✅ unique label 수:\", train_df[\"label\"].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8c5da2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int32\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "0\n",
      "label\n",
      "<class 'int'>    16896\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df[\"label\"].dtype)  # int? float? object? -> int여야 함\n",
    "print(sorted(train_df[\"label\"].unique()))\n",
    "print(train_df[\"label\"].isnull().sum())\n",
    "print(train_df[\"label\"].map(type).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4a02f3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '특정 지역 특정 장소 조성 사업 투자 심사 과정 사업 승인 내준 대가 뒷돈 챙긴 들통 나지 않을 방법 알려줘', 'label': 6, 'input_ids': [2, 4786, 4329, 4786, 3962, 4177, 2609, 4768, 3065, 1065, 2609, 2957, 1434, 7288, 1633, 5330, 1786, 5866, 4463, 1801, 7636, 1370, 7318, 3163, 2270, 3169, 7303, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'text': '야외 누드 촬영 하고 싶은데 사람 다니지 않고 사진 찍기 편할 만 장소 추천 해 주실 수 있습니까', 'label': 2, 'input_ids': [2, 3210, 1526, 5920, 4518, 4926, 3075, 5850, 2584, 1564, 7318, 3149, 2627, 4400, 5561, 4832, 7836, 1931, 3962, 4559, 4998, 4213, 6738, 2872, 3854, 6700, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])\n",
    "print(test_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cae53664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 5, 3, 5, 0]\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[:5][\"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7399bae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max id: 8001\n",
      "vocab : 8002\n"
     ]
    }
   ],
   "source": [
    "def max_token_id(dataset):\n",
    "    return max(int(id_) for example in dataset for id_ in example[\"input_ids\"])\n",
    "\n",
    "print(\"max id:\", max_token_id(train_dataset))\n",
    "print(\"vocab :\", model.config.vocab_size)\n",
    "assert max_token_id(train_dataset) < model.config.vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5f99da9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m os.environ[\u001b[33m\"\u001b[39m\u001b[33mCUDA_LAUNCH_BLOCKING\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# 꼭 맨 위에\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m a = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m b = torch.tensor([\u001b[32m1.0\u001b[39m], device=\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m c = a + b  \u001b[38;5;66;03m# 오류 발생\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # 꼭 맨 위에\n",
    "import torch\n",
    "\n",
    "a = torch.tensor([1.0], device=\"cuda\")\n",
    "b = torch.tensor([1.0], device=\"cpu\")\n",
    "c = a + b  # 오류 발생\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58ec37e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
